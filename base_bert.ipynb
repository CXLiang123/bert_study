{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "import torch\n",
    "import time \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig, BertAdam\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import *\n",
    " \n",
    "path = \"THUCNews/data/\"\n",
    "bert_path = \"bert_pretrain/\"\n",
    "tokenizer = BertTokenizer(vocab_file=bert_path + \"vocab.txt\")  # 初始化分词器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中华女子学院：本科层次仅1专业招男生\t3\n",
      "\n",
      "两天价网站背后重重迷雾：做个网站究竟要多少钱\t4\n",
      "\n",
      "东5环海棠公社230-290平2居准现房98折优惠\t1\n",
      "\n",
      "卡佩罗：告诉你德国脚生猛的原因 不希望英德战踢点球\t7\n",
      "\n",
      "82岁老太为学生做饭扫地44年获授港大荣誉院士\t5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = []     # input char ids\n",
    "input_types = []   # segment ids\n",
    "input_masks = []   # attention mask\n",
    "label = []         # 标签\n",
    "pad_size = 32      # 也称为 max_len (前期统计分析，文本长度最大值为38，取32即可覆盖99%)\n",
    "\n",
    "with open(path + \"train.txt\", encoding='utf-8') as f:\n",
    "    for i in range(5):  # 读取前5行内容\n",
    "        line = f.readline()\n",
    "        print(line)\n",
    "        l=line\n",
    "        #x1, y = line.split('/t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82岁老太为学生做饭扫地44年获授港大荣誉院士\n",
      "5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l\n",
    "x1, y = l.split('\\t')\n",
    "print(x1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180000it [00:16, 10904.05it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids = []     # input char ids\n",
    "input_types = []   # segment ids\n",
    "input_masks = []   # attention mask\n",
    "label = []         # 标签\n",
    "pad_size = 32      # 也称为 max_len (前期统计分析，文本长度最大值为38，取32即可覆盖99%)\n",
    "\n",
    "with open(path + \"train.txt\", encoding='utf-8') as f:\n",
    "    for i, l in tqdm(enumerate(f)): \n",
    "        x1, y = l.strip().split('\\t')\n",
    "        x1 = tokenizer.tokenize(x1)\n",
    "        tokens = [\"[CLS]\"] + x1 + [\"[SEP]\"]\n",
    "        \n",
    "        # 得到input_id, seg_id, att_mask\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        types = [0] *(len(ids))\n",
    "        masks = [1] * len(ids)\n",
    "        # 短则补齐，长则切断\n",
    "        if len(ids) < pad_size:\n",
    "            types = types + [1] * (pad_size - len(ids))  # mask部分 segment置为1\n",
    "            masks = masks + [0] * (pad_size - len(ids))\n",
    "            ids = ids + [0] * (pad_size - len(ids))\n",
    "        else:\n",
    "            types = types[:pad_size]\n",
    "            masks = masks[:pad_size]\n",
    "            ids = ids[:pad_size]\n",
    "        input_ids.append(ids)\n",
    "        input_types.append(types)\n",
    "        input_masks.append(masks)\n",
    "#         print(len(ids), len(masks), len(types)) \n",
    "        assert len(ids) == len(masks) == len(types) == pad_size\n",
    "        label.append([int(y)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[179616, 142169, 777, 82271, 110598, 76909, 169545, 31072, 103671, 126007]\n",
      "(144000, 32) (144000, 32) (144000, 32) (144000, 1)\n",
      "(36000, 32) (36000, 32) (36000, 32) (36000, 1)\n"
     ]
    }
   ],
   "source": [
    "# 随机打乱索引\n",
    "random_order = list(range(len(input_ids)))\n",
    "np.random.seed(2020)   # 固定种子\n",
    "np.random.shuffle(random_order)\n",
    "print(random_order[:10])\n",
    " \n",
    "# 4:1 划分训练集和测试集   取出打乱后的前 80% 的索引\n",
    "input_ids_train = np.array([input_ids[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "input_types_train = np.array([input_types[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "input_masks_train = np.array([input_masks[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "y_train = np.array([label[i] for i in random_order[:int(len(input_ids) * 0.8)]])\n",
    "print(input_ids_train.shape, input_types_train.shape, input_masks_train.shape, y_train.shape)\n",
    " \n",
    "input_ids_test = np.array([input_ids[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "input_types_test = np.array([input_types[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "input_masks_test = np.array([input_masks[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "y_test = np.array([label[i] for i in random_order[int(len(input_ids) * 0.8):]])\n",
    "print(input_ids_test.shape, input_types_test.shape, input_masks_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_data = TensorDataset(torch.LongTensor(input_ids_train), \n",
    "                           torch.LongTensor(input_types_train), \n",
    "                           torch.LongTensor(input_masks_train), \n",
    "                           torch.LongTensor(y_train))\n",
    "train_sampler = RandomSampler(train_data)  \n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    " \n",
    "test_data = TensorDataset(torch.LongTensor(input_ids_test), \n",
    "                          torch.LongTensor(input_types_test), \n",
    "                         torch.LongTensor(input_masks_test),\n",
    "                          torch.LongTensor(y_test))\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_path)  # /bert_pretrain/\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True  # 每个参数都要 求梯度\n",
    "        self.fc = nn.Linear(768, 10)   # 768 -> 2\n",
    " \n",
    "    def forward(self, x):\n",
    "        context = x[0]  # 输入的句子   (ids, seq_len, mask)\n",
    "        types = x[1]\n",
    "        mask = x[2]  # 对padding部分进行mask，和句子相同size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n",
    "\n",
    "        # (batch_size, sequence_length, hidden_size) 批量大小 句子的最大长度 bert的隐藏状态大小\n",
    "        #  (batch_size, hidden_size)  第二个\n",
    "        _, pooled = self.bert(context, token_type_ids=types, \n",
    "                              attention_mask=mask, \n",
    "                              output_all_encoded_layers=False) # 控制是否输出所有encoder层的结果\n",
    "        out = self.fc(pooled)   # 得到10分类\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=768, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model().to(DEVICE)\n",
    "print(model) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())  # 模型参数名字列表\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    " \n",
    "NUM_EPOCHS = 3\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=0.05,\n",
    "                     t_total=len(train_loader) * NUM_EPOCHS\n",
    "                    )\n",
    " \n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)   # 简单起见，可用这一行代码完事\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[1][1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):   # 训练模型\n",
    "    model.train()\n",
    "    best_acc = 0.0 \n",
    "    for batch_idx, (x1,x2,x3, y) in enumerate(train_loader):\n",
    "        start_time = time.time()\n",
    "        x1,x2,x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "        y_pred = model([x1, x2, x3])  # 得到预测结果\n",
    "        model.zero_grad()             # 梯度清零\n",
    "        loss = F.cross_entropy(y_pred, y.squeeze())  # 得到loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx + 1) % 100 == 0:    # 打印loss\n",
    "            print('Train Epoch: {} [{}/{} ({:.2f}%)]tLoss: {:.6f}'.format(epoch, (batch_idx+1) * len(x1), \n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader), \n",
    "                                                                           loss.item()))  # 记得为loss.item()\n",
    " \n",
    "def test(model, device, test_loader):    # 测试模型, 得到测试集评估结果\n",
    "    model.eval()\n",
    "    test_loss = 0.0 \n",
    "    acc = 0 \n",
    "    for batch_idx, (x1,x2,x3, y) in enumerate(test_loader):\n",
    "        x1,x2,x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_ = model([x1,x2,x3])\n",
    "        test_loss += F.cross_entropy(y_, y.squeeze())\n",
    "        pred = y_.max(-1, keepdim=True)[1]   # .max(): 2输出，分别为最大值和最大值的index\n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()    # 记得加item()\n",
    "    test_loss /= len(test_loader)\n",
    "    print('nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "          test_loss, acc, len(test_loader.dataset),\n",
    "          100. * acc / len(test_loader.dataset)))\n",
    "    return acc / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/daiyuanyuan/lib/python3.9/site-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1600/144000 (1.10%)]tLoss: 1.775924\n",
      "Train Epoch: 1 [3200/144000 (2.21%)]tLoss: 0.819517\n",
      "Train Epoch: 1 [4800/144000 (3.32%)]tLoss: 0.540012\n",
      "Train Epoch: 1 [6400/144000 (4.43%)]tLoss: 0.330524\n",
      "Train Epoch: 1 [8000/144000 (5.54%)]tLoss: 0.498789\n",
      "Train Epoch: 1 [9600/144000 (6.66%)]tLoss: 0.462453\n",
      "Train Epoch: 1 [11200/144000 (7.77%)]tLoss: 0.288453\n",
      "Train Epoch: 1 [12800/144000 (8.88%)]tLoss: 0.579245\n",
      "Train Epoch: 1 [14400/144000 (9.99%)]tLoss: 0.111901\n",
      "Train Epoch: 1 [16000/144000 (11.10%)]tLoss: 0.192260\n",
      "Train Epoch: 1 [17600/144000 (12.21%)]tLoss: 0.223583\n",
      "Train Epoch: 1 [19200/144000 (13.32%)]tLoss: 0.421820\n",
      "Train Epoch: 1 [20800/144000 (14.43%)]tLoss: 0.873880\n",
      "Train Epoch: 1 [22400/144000 (15.54%)]tLoss: 0.254541\n",
      "Train Epoch: 1 [24000/144000 (16.66%)]tLoss: 0.133459\n",
      "Train Epoch: 1 [25600/144000 (17.77%)]tLoss: 0.260614\n",
      "Train Epoch: 1 [27200/144000 (18.88%)]tLoss: 0.145063\n",
      "Train Epoch: 1 [28800/144000 (19.99%)]tLoss: 0.177573\n",
      "Train Epoch: 1 [30400/144000 (21.10%)]tLoss: 0.185692\n",
      "Train Epoch: 1 [32000/144000 (22.21%)]tLoss: 0.067270\n",
      "Train Epoch: 1 [33600/144000 (23.32%)]tLoss: 0.735496\n",
      "Train Epoch: 1 [35200/144000 (24.43%)]tLoss: 0.273616\n",
      "Train Epoch: 1 [36800/144000 (25.54%)]tLoss: 0.106287\n",
      "Train Epoch: 1 [38400/144000 (26.66%)]tLoss: 0.191645\n",
      "Train Epoch: 1 [40000/144000 (27.77%)]tLoss: 0.210293\n",
      "Train Epoch: 1 [41600/144000 (28.88%)]tLoss: 0.658221\n",
      "Train Epoch: 1 [43200/144000 (29.99%)]tLoss: 0.071077\n",
      "Train Epoch: 1 [44800/144000 (31.10%)]tLoss: 0.389946\n",
      "Train Epoch: 1 [46400/144000 (32.21%)]tLoss: 0.243630\n",
      "Train Epoch: 1 [48000/144000 (33.32%)]tLoss: 0.076274\n",
      "Train Epoch: 1 [49600/144000 (34.43%)]tLoss: 0.420052\n",
      "Train Epoch: 1 [51200/144000 (35.54%)]tLoss: 0.214863\n",
      "Train Epoch: 1 [52800/144000 (36.66%)]tLoss: 0.505179\n",
      "Train Epoch: 1 [54400/144000 (37.77%)]tLoss: 0.013983\n",
      "Train Epoch: 1 [56000/144000 (38.88%)]tLoss: 0.048795\n",
      "Train Epoch: 1 [57600/144000 (39.99%)]tLoss: 0.169191\n",
      "Train Epoch: 1 [59200/144000 (41.10%)]tLoss: 0.391848\n",
      "Train Epoch: 1 [60800/144000 (42.21%)]tLoss: 0.247750\n",
      "Train Epoch: 1 [62400/144000 (43.32%)]tLoss: 0.066445\n",
      "Train Epoch: 1 [64000/144000 (44.43%)]tLoss: 0.265523\n",
      "Train Epoch: 1 [65600/144000 (45.54%)]tLoss: 0.051255\n",
      "Train Epoch: 1 [67200/144000 (46.66%)]tLoss: 0.106725\n",
      "Train Epoch: 1 [68800/144000 (47.77%)]tLoss: 0.870451\n",
      "Train Epoch: 1 [70400/144000 (48.88%)]tLoss: 0.288928\n",
      "Train Epoch: 1 [72000/144000 (49.99%)]tLoss: 0.134926\n",
      "Train Epoch: 1 [73600/144000 (51.10%)]tLoss: 0.204199\n",
      "Train Epoch: 1 [75200/144000 (52.21%)]tLoss: 0.210938\n",
      "Train Epoch: 1 [76800/144000 (53.32%)]tLoss: 0.246984\n",
      "Train Epoch: 1 [78400/144000 (54.43%)]tLoss: 0.074800\n",
      "Train Epoch: 1 [80000/144000 (55.54%)]tLoss: 0.453708\n",
      "Train Epoch: 1 [81600/144000 (56.66%)]tLoss: 0.220975\n",
      "Train Epoch: 1 [83200/144000 (57.77%)]tLoss: 0.021318\n",
      "Train Epoch: 1 [84800/144000 (58.88%)]tLoss: 0.051778\n",
      "Train Epoch: 1 [86400/144000 (59.99%)]tLoss: 0.208767\n",
      "Train Epoch: 1 [88000/144000 (61.10%)]tLoss: 0.224471\n",
      "Train Epoch: 1 [89600/144000 (62.21%)]tLoss: 0.526757\n",
      "Train Epoch: 1 [91200/144000 (63.32%)]tLoss: 0.031474\n",
      "Train Epoch: 1 [92800/144000 (64.43%)]tLoss: 0.039613\n",
      "Train Epoch: 1 [94400/144000 (65.54%)]tLoss: 0.042050\n",
      "Train Epoch: 1 [96000/144000 (66.66%)]tLoss: 0.221292\n",
      "Train Epoch: 1 [97600/144000 (67.77%)]tLoss: 0.667850\n",
      "Train Epoch: 1 [99200/144000 (68.88%)]tLoss: 0.181534\n",
      "Train Epoch: 1 [100800/144000 (69.99%)]tLoss: 0.094870\n",
      "Train Epoch: 1 [102400/144000 (71.10%)]tLoss: 0.099794\n",
      "Train Epoch: 1 [104000/144000 (72.21%)]tLoss: 0.278911\n",
      "Train Epoch: 1 [105600/144000 (73.32%)]tLoss: 0.043722\n",
      "Train Epoch: 1 [107200/144000 (74.43%)]tLoss: 0.460498\n",
      "Train Epoch: 1 [108800/144000 (75.54%)]tLoss: 0.404884\n",
      "Train Epoch: 1 [110400/144000 (76.66%)]tLoss: 0.142684\n",
      "Train Epoch: 1 [112000/144000 (77.77%)]tLoss: 0.082659\n",
      "Train Epoch: 1 [113600/144000 (78.88%)]tLoss: 0.126503\n",
      "Train Epoch: 1 [115200/144000 (79.99%)]tLoss: 0.115707\n",
      "Train Epoch: 1 [116800/144000 (81.10%)]tLoss: 0.018126\n",
      "Train Epoch: 1 [118400/144000 (82.21%)]tLoss: 0.031032\n",
      "Train Epoch: 1 [120000/144000 (83.32%)]tLoss: 0.098791\n",
      "Train Epoch: 1 [121600/144000 (84.43%)]tLoss: 0.257218\n",
      "Train Epoch: 1 [123200/144000 (85.54%)]tLoss: 0.332964\n",
      "Train Epoch: 1 [124800/144000 (86.66%)]tLoss: 0.095064\n",
      "Train Epoch: 1 [126400/144000 (87.77%)]tLoss: 0.788573\n",
      "Train Epoch: 1 [128000/144000 (88.88%)]tLoss: 0.051618\n",
      "Train Epoch: 1 [129600/144000 (89.99%)]tLoss: 0.196830\n",
      "Train Epoch: 1 [131200/144000 (91.10%)]tLoss: 0.366971\n",
      "Train Epoch: 1 [132800/144000 (92.21%)]tLoss: 0.102129\n",
      "Train Epoch: 1 [134400/144000 (93.32%)]tLoss: 0.417899\n",
      "Train Epoch: 1 [136000/144000 (94.43%)]tLoss: 0.127756\n",
      "Train Epoch: 1 [137600/144000 (95.54%)]tLoss: 0.526590\n",
      "Train Epoch: 1 [139200/144000 (96.66%)]tLoss: 0.083882\n",
      "Train Epoch: 1 [140800/144000 (97.77%)]tLoss: 0.051089\n",
      "Train Epoch: 1 [142400/144000 (98.88%)]tLoss: 0.139970\n",
      "Train Epoch: 1 [144000/144000 (99.99%)]tLoss: 0.047539\n",
      "nTest set: Average loss: 0.1787, Accuracy: 33886/36000 (94.13%)\n",
      "acc is: 0.9413, best acc is 0.9413n\n",
      "Train Epoch: 2 [1600/144000 (1.10%)]tLoss: 0.060465\n",
      "Train Epoch: 2 [3200/144000 (2.21%)]tLoss: 0.131212\n",
      "Train Epoch: 2 [4800/144000 (3.32%)]tLoss: 0.009930\n",
      "Train Epoch: 2 [6400/144000 (4.43%)]tLoss: 0.006548\n",
      "Train Epoch: 2 [8000/144000 (5.54%)]tLoss: 0.143068\n",
      "Train Epoch: 2 [9600/144000 (6.66%)]tLoss: 0.259653\n",
      "Train Epoch: 2 [11200/144000 (7.77%)]tLoss: 0.089060\n",
      "Train Epoch: 2 [12800/144000 (8.88%)]tLoss: 0.185005\n",
      "Train Epoch: 2 [14400/144000 (9.99%)]tLoss: 0.053487\n",
      "Train Epoch: 2 [16000/144000 (11.10%)]tLoss: 0.048162\n",
      "Train Epoch: 2 [17600/144000 (12.21%)]tLoss: 0.083038\n",
      "Train Epoch: 2 [19200/144000 (13.32%)]tLoss: 0.079768\n",
      "Train Epoch: 2 [20800/144000 (14.43%)]tLoss: 0.056689\n",
      "Train Epoch: 2 [22400/144000 (15.54%)]tLoss: 0.508944\n",
      "Train Epoch: 2 [24000/144000 (16.66%)]tLoss: 0.010208\n",
      "Train Epoch: 2 [25600/144000 (17.77%)]tLoss: 0.046960\n",
      "Train Epoch: 2 [27200/144000 (18.88%)]tLoss: 0.238109\n",
      "Train Epoch: 2 [28800/144000 (19.99%)]tLoss: 0.006626\n",
      "Train Epoch: 2 [30400/144000 (21.10%)]tLoss: 0.053372\n",
      "Train Epoch: 2 [32000/144000 (22.21%)]tLoss: 0.259835\n",
      "Train Epoch: 2 [33600/144000 (23.32%)]tLoss: 0.049564\n",
      "Train Epoch: 2 [35200/144000 (24.43%)]tLoss: 0.031979\n",
      "Train Epoch: 2 [36800/144000 (25.54%)]tLoss: 0.236889\n",
      "Train Epoch: 2 [38400/144000 (26.66%)]tLoss: 0.031132\n",
      "Train Epoch: 2 [40000/144000 (27.77%)]tLoss: 0.074623\n",
      "Train Epoch: 2 [41600/144000 (28.88%)]tLoss: 0.274447\n",
      "Train Epoch: 2 [43200/144000 (29.99%)]tLoss: 0.030332\n",
      "Train Epoch: 2 [44800/144000 (31.10%)]tLoss: 0.435019\n",
      "Train Epoch: 2 [46400/144000 (32.21%)]tLoss: 0.066339\n",
      "Train Epoch: 2 [48000/144000 (33.32%)]tLoss: 0.033792\n",
      "Train Epoch: 2 [49600/144000 (34.43%)]tLoss: 0.075604\n",
      "Train Epoch: 2 [51200/144000 (35.54%)]tLoss: 0.012805\n",
      "Train Epoch: 2 [52800/144000 (36.66%)]tLoss: 0.261542\n",
      "Train Epoch: 2 [54400/144000 (37.77%)]tLoss: 0.058707\n",
      "Train Epoch: 2 [56000/144000 (38.88%)]tLoss: 0.008677\n",
      "Train Epoch: 2 [57600/144000 (39.99%)]tLoss: 0.057254\n",
      "Train Epoch: 2 [59200/144000 (41.10%)]tLoss: 0.033631\n",
      "Train Epoch: 2 [60800/144000 (42.21%)]tLoss: 0.019605\n",
      "Train Epoch: 2 [62400/144000 (43.32%)]tLoss: 0.069553\n",
      "Train Epoch: 2 [64000/144000 (44.43%)]tLoss: 0.377447\n",
      "Train Epoch: 2 [65600/144000 (45.54%)]tLoss: 0.005252\n",
      "Train Epoch: 2 [67200/144000 (46.66%)]tLoss: 0.008818\n",
      "Train Epoch: 2 [68800/144000 (47.77%)]tLoss: 0.019724\n",
      "Train Epoch: 2 [70400/144000 (48.88%)]tLoss: 0.066668\n",
      "Train Epoch: 2 [72000/144000 (49.99%)]tLoss: 0.018230\n",
      "Train Epoch: 2 [73600/144000 (51.10%)]tLoss: 0.004451\n",
      "Train Epoch: 2 [75200/144000 (52.21%)]tLoss: 0.101682\n",
      "Train Epoch: 2 [76800/144000 (53.32%)]tLoss: 0.005407\n",
      "Train Epoch: 2 [78400/144000 (54.43%)]tLoss: 0.117298\n",
      "Train Epoch: 2 [80000/144000 (55.54%)]tLoss: 0.009492\n",
      "Train Epoch: 2 [81600/144000 (56.66%)]tLoss: 0.027395\n",
      "Train Epoch: 2 [83200/144000 (57.77%)]tLoss: 0.003228\n",
      "Train Epoch: 2 [84800/144000 (58.88%)]tLoss: 0.062911\n",
      "Train Epoch: 2 [86400/144000 (59.99%)]tLoss: 0.019148\n",
      "Train Epoch: 2 [88000/144000 (61.10%)]tLoss: 0.557369\n",
      "Train Epoch: 2 [89600/144000 (62.21%)]tLoss: 0.118473\n",
      "Train Epoch: 2 [91200/144000 (63.32%)]tLoss: 0.261443\n",
      "Train Epoch: 2 [92800/144000 (64.43%)]tLoss: 0.049502\n",
      "Train Epoch: 2 [94400/144000 (65.54%)]tLoss: 0.088318\n",
      "Train Epoch: 2 [96000/144000 (66.66%)]tLoss: 0.291295\n",
      "Train Epoch: 2 [97600/144000 (67.77%)]tLoss: 0.057409\n",
      "Train Epoch: 2 [99200/144000 (68.88%)]tLoss: 0.351242\n",
      "Train Epoch: 2 [100800/144000 (69.99%)]tLoss: 0.006605\n",
      "Train Epoch: 2 [102400/144000 (71.10%)]tLoss: 0.128964\n",
      "Train Epoch: 2 [104000/144000 (72.21%)]tLoss: 0.215509\n",
      "Train Epoch: 2 [105600/144000 (73.32%)]tLoss: 0.006607\n",
      "Train Epoch: 2 [107200/144000 (74.43%)]tLoss: 0.038477\n",
      "Train Epoch: 2 [108800/144000 (75.54%)]tLoss: 0.179820\n",
      "Train Epoch: 2 [110400/144000 (76.66%)]tLoss: 0.105454\n",
      "Train Epoch: 2 [112000/144000 (77.77%)]tLoss: 0.093689\n",
      "Train Epoch: 2 [113600/144000 (78.88%)]tLoss: 0.014838\n",
      "Train Epoch: 2 [115200/144000 (79.99%)]tLoss: 0.245205\n",
      "Train Epoch: 2 [116800/144000 (81.10%)]tLoss: 0.363251\n",
      "Train Epoch: 2 [118400/144000 (82.21%)]tLoss: 0.508258\n",
      "Train Epoch: 2 [120000/144000 (83.32%)]tLoss: 0.056239\n",
      "Train Epoch: 2 [121600/144000 (84.43%)]tLoss: 0.337172\n",
      "Train Epoch: 2 [123200/144000 (85.54%)]tLoss: 0.068859\n",
      "Train Epoch: 2 [124800/144000 (86.66%)]tLoss: 0.332397\n",
      "Train Epoch: 2 [126400/144000 (87.77%)]tLoss: 0.002466\n",
      "Train Epoch: 2 [128000/144000 (88.88%)]tLoss: 0.143986\n",
      "Train Epoch: 2 [129600/144000 (89.99%)]tLoss: 0.070604\n",
      "Train Epoch: 2 [131200/144000 (91.10%)]tLoss: 0.018090\n",
      "Train Epoch: 2 [132800/144000 (92.21%)]tLoss: 0.389721\n",
      "Train Epoch: 2 [134400/144000 (93.32%)]tLoss: 0.043308\n",
      "Train Epoch: 2 [136000/144000 (94.43%)]tLoss: 0.050517\n",
      "Train Epoch: 2 [137600/144000 (95.54%)]tLoss: 0.022912\n",
      "Train Epoch: 2 [139200/144000 (96.66%)]tLoss: 0.102646\n",
      "Train Epoch: 2 [140800/144000 (97.77%)]tLoss: 0.004603\n",
      "Train Epoch: 2 [142400/144000 (98.88%)]tLoss: 0.026462\n",
      "Train Epoch: 2 [144000/144000 (99.99%)]tLoss: 0.471888\n",
      "nTest set: Average loss: 0.1717, Accuracy: 34085/36000 (94.68%)\n",
      "acc is: 0.9468, best acc is 0.9468n\n",
      "Train Epoch: 3 [1600/144000 (1.10%)]tLoss: 0.012098\n",
      "Train Epoch: 3 [3200/144000 (2.21%)]tLoss: 0.197092\n",
      "Train Epoch: 3 [4800/144000 (3.32%)]tLoss: 0.039155\n",
      "Train Epoch: 3 [6400/144000 (4.43%)]tLoss: 0.152632\n",
      "Train Epoch: 3 [8000/144000 (5.54%)]tLoss: 0.006415\n",
      "Train Epoch: 3 [9600/144000 (6.66%)]tLoss: 0.010171\n",
      "Train Epoch: 3 [11200/144000 (7.77%)]tLoss: 0.162303\n",
      "Train Epoch: 3 [12800/144000 (8.88%)]tLoss: 0.007495\n",
      "Train Epoch: 3 [14400/144000 (9.99%)]tLoss: 0.136048\n",
      "Train Epoch: 3 [16000/144000 (11.10%)]tLoss: 0.010698\n",
      "Train Epoch: 3 [17600/144000 (12.21%)]tLoss: 0.591589\n",
      "Train Epoch: 3 [19200/144000 (13.32%)]tLoss: 0.064256\n",
      "Train Epoch: 3 [20800/144000 (14.43%)]tLoss: 0.074883\n",
      "Train Epoch: 3 [22400/144000 (15.54%)]tLoss: 0.001353\n",
      "Train Epoch: 3 [24000/144000 (16.66%)]tLoss: 0.112825\n",
      "Train Epoch: 3 [25600/144000 (17.77%)]tLoss: 0.032641\n",
      "Train Epoch: 3 [27200/144000 (18.88%)]tLoss: 0.005765\n",
      "Train Epoch: 3 [28800/144000 (19.99%)]tLoss: 0.079833\n",
      "Train Epoch: 3 [30400/144000 (21.10%)]tLoss: 0.052378\n",
      "Train Epoch: 3 [32000/144000 (22.21%)]tLoss: 0.026200\n",
      "Train Epoch: 3 [33600/144000 (23.32%)]tLoss: 0.047380\n",
      "Train Epoch: 3 [35200/144000 (24.43%)]tLoss: 0.022073\n",
      "Train Epoch: 3 [36800/144000 (25.54%)]tLoss: 0.228802\n",
      "Train Epoch: 3 [38400/144000 (26.66%)]tLoss: 0.001821\n",
      "Train Epoch: 3 [40000/144000 (27.77%)]tLoss: 0.081470\n",
      "Train Epoch: 3 [41600/144000 (28.88%)]tLoss: 0.002354\n",
      "Train Epoch: 3 [43200/144000 (29.99%)]tLoss: 0.226959\n",
      "Train Epoch: 3 [44800/144000 (31.10%)]tLoss: 0.002895\n",
      "Train Epoch: 3 [46400/144000 (32.21%)]tLoss: 0.020775\n",
      "Train Epoch: 3 [48000/144000 (33.32%)]tLoss: 0.135995\n",
      "Train Epoch: 3 [49600/144000 (34.43%)]tLoss: 0.027398\n",
      "Train Epoch: 3 [51200/144000 (35.54%)]tLoss: 0.012190\n",
      "Train Epoch: 3 [52800/144000 (36.66%)]tLoss: 0.046148\n",
      "Train Epoch: 3 [54400/144000 (37.77%)]tLoss: 0.094552\n",
      "Train Epoch: 3 [56000/144000 (38.88%)]tLoss: 0.413467\n",
      "Train Epoch: 3 [57600/144000 (39.99%)]tLoss: 0.218988\n",
      "Train Epoch: 3 [59200/144000 (41.10%)]tLoss: 0.286459\n",
      "Train Epoch: 3 [60800/144000 (42.21%)]tLoss: 0.007349\n",
      "Train Epoch: 3 [62400/144000 (43.32%)]tLoss: 0.003630\n",
      "Train Epoch: 3 [64000/144000 (44.43%)]tLoss: 0.063320\n",
      "Train Epoch: 3 [65600/144000 (45.54%)]tLoss: 0.161313\n",
      "Train Epoch: 3 [67200/144000 (46.66%)]tLoss: 0.184528\n",
      "Train Epoch: 3 [68800/144000 (47.77%)]tLoss: 0.051170\n",
      "Train Epoch: 3 [70400/144000 (48.88%)]tLoss: 0.002992\n",
      "Train Epoch: 3 [72000/144000 (49.99%)]tLoss: 0.016543\n",
      "Train Epoch: 3 [73600/144000 (51.10%)]tLoss: 0.154307\n",
      "Train Epoch: 3 [75200/144000 (52.21%)]tLoss: 0.458965\n",
      "Train Epoch: 3 [76800/144000 (53.32%)]tLoss: 0.066577\n",
      "Train Epoch: 3 [78400/144000 (54.43%)]tLoss: 0.005892\n",
      "Train Epoch: 3 [80000/144000 (55.54%)]tLoss: 0.008057\n",
      "Train Epoch: 3 [81600/144000 (56.66%)]tLoss: 0.011939\n",
      "Train Epoch: 3 [83200/144000 (57.77%)]tLoss: 0.003851\n",
      "Train Epoch: 3 [84800/144000 (58.88%)]tLoss: 0.201289\n",
      "Train Epoch: 3 [86400/144000 (59.99%)]tLoss: 0.119197\n",
      "Train Epoch: 3 [88000/144000 (61.10%)]tLoss: 0.046035\n",
      "Train Epoch: 3 [89600/144000 (62.21%)]tLoss: 0.001546\n",
      "Train Epoch: 3 [91200/144000 (63.32%)]tLoss: 0.102158\n",
      "Train Epoch: 3 [92800/144000 (64.43%)]tLoss: 0.155126\n",
      "Train Epoch: 3 [94400/144000 (65.54%)]tLoss: 0.015240\n",
      "Train Epoch: 3 [96000/144000 (66.66%)]tLoss: 0.012441\n",
      "Train Epoch: 3 [97600/144000 (67.77%)]tLoss: 0.236827\n",
      "Train Epoch: 3 [99200/144000 (68.88%)]tLoss: 0.002072\n",
      "Train Epoch: 3 [100800/144000 (69.99%)]tLoss: 0.024527\n",
      "Train Epoch: 3 [102400/144000 (71.10%)]tLoss: 0.311565\n",
      "Train Epoch: 3 [104000/144000 (72.21%)]tLoss: 0.047635\n",
      "Train Epoch: 3 [105600/144000 (73.32%)]tLoss: 0.008245\n",
      "Train Epoch: 3 [107200/144000 (74.43%)]tLoss: 0.008905\n",
      "Train Epoch: 3 [108800/144000 (75.54%)]tLoss: 0.036472\n",
      "Train Epoch: 3 [110400/144000 (76.66%)]tLoss: 0.180358\n",
      "Train Epoch: 3 [112000/144000 (77.77%)]tLoss: 0.337194\n",
      "Train Epoch: 3 [113600/144000 (78.88%)]tLoss: 0.006002\n",
      "Train Epoch: 3 [115200/144000 (79.99%)]tLoss: 0.119766\n",
      "Train Epoch: 3 [116800/144000 (81.10%)]tLoss: 0.001352\n",
      "Train Epoch: 3 [118400/144000 (82.21%)]tLoss: 0.049078\n",
      "Train Epoch: 3 [120000/144000 (83.32%)]tLoss: 0.017274\n",
      "Train Epoch: 3 [121600/144000 (84.43%)]tLoss: 0.066322\n",
      "Train Epoch: 3 [123200/144000 (85.54%)]tLoss: 0.004243\n",
      "Train Epoch: 3 [124800/144000 (86.66%)]tLoss: 0.013620\n",
      "Train Epoch: 3 [126400/144000 (87.77%)]tLoss: 0.010299\n",
      "Train Epoch: 3 [128000/144000 (88.88%)]tLoss: 0.019331\n",
      "Train Epoch: 3 [129600/144000 (89.99%)]tLoss: 0.014129\n",
      "Train Epoch: 3 [131200/144000 (91.10%)]tLoss: 0.062892\n",
      "Train Epoch: 3 [132800/144000 (92.21%)]tLoss: 0.002667\n",
      "Train Epoch: 3 [134400/144000 (93.32%)]tLoss: 0.013460\n",
      "Train Epoch: 3 [136000/144000 (94.43%)]tLoss: 0.036924\n",
      "Train Epoch: 3 [137600/144000 (95.54%)]tLoss: 0.276277\n",
      "Train Epoch: 3 [139200/144000 (96.66%)]tLoss: 0.012187\n",
      "Train Epoch: 3 [140800/144000 (97.77%)]tLoss: 0.010162\n",
      "Train Epoch: 3 [142400/144000 (98.88%)]tLoss: 0.242519\n",
      "Train Epoch: 3 [144000/144000 (99.99%)]tLoss: 0.439636\n",
      "nTest set: Average loss: 0.1841, Accuracy: 34150/36000 (94.86%)\n",
      "acc is: 0.9486, best acc is 0.9486n\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0 \n",
    "PATH = 'roberta_model.pth'  # 定义模型保存路径\n",
    "for epoch in range(1, NUM_EPOCHS+1):  # 3个epoch\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    acc = test(model, DEVICE, test_loader)\n",
    "    if best_acc < acc: \n",
    "        best_acc = acc \n",
    "        torch.save(model.state_dict(), PATH)  # 保存最优模型\n",
    "    print(\"acc is: {:.4f}, best acc is {:.4f}n\".format(acc, best_acc))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nTest set: Average loss: 0.1841, Accuracy: 34150/36000 (94.86%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 测试集提交\\nPATH = \"roberta_model.pth\"\\nmodel.load_state_dict(torch.load(PATH))\\ndef test_for_submit(model, device, test_loader):    # 测试模型\\n    model.eval()\\n    preds = []\\n    for batch_idx, (x1,x2,x3) in tqdm(enumerate(test_loader)):\\n        x1,x2,x3 = x1.to(device), x2.to(device), x3.to(device)\\n        with torch.no_grad():\\n            y_ = model([x1,x2,x3])\\n        pred = y_.max(-1, keepdim=True)[1].squeeze().cpu().tolist()   \\n        # .max() 2输出，分别为最大值和最大值的index\\n        preds.extend(pred) \\n    return preds \\npreds = test_for_submit(model, DEVICE, test_loader)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(\"roberta_model.pth\"))\n",
    "acc = test(model, DEVICE, test_loader)\n",
    " \n",
    "# 如果打比赛的话，下面代码也可参考\n",
    "\"\"\"\n",
    "# 测试集提交\n",
    "PATH = \"roberta_model.pth\"\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "def test_for_submit(model, device, test_loader):    # 测试模型\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for batch_idx, (x1,x2,x3) in tqdm(enumerate(test_loader)):\n",
    "        x1,x2,x3 = x1.to(device), x2.to(device), x3.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_ = model([x1,x2,x3])\n",
    "        pred = y_.max(-1, keepdim=True)[1].squeeze().cpu().tolist()   \n",
    "        # .max() 2输出，分别为最大值和最大值的index\n",
    "        preds.extend(pred) \n",
    "    return preds \n",
    "preds = test_for_submit(model, DEVICE, test_loader)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daiyuanyuan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
